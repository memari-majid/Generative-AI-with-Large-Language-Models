Knowledge Base Builder with Code Llama
This project is a Streamlit web application that automatically reads all files in the current directory, builds a knowledge base using embeddings and vector stores, and allows users to interactively ask questions about the content using the Code Llama language model.

Table of Contents
Features
How It Works
Requirements
Installation
Usage
Technical Details
Notes
License
Features
Automatically reads and processes all files in the current directory and subdirectories.
Supports all file types that can be read as text.
Builds a knowledge base using embeddings generated by Hugging Face models.
Uses Code Llama for language understanding and question answering.
Provides a web-based user interface using Streamlit.
Displays source documents related to the answers.
How It Works
Upon running, the application performs the following steps:

File Loading: Recursively scans the current directory and reads all files as text.
Document Splitting: Splits the loaded text into manageable chunks for processing.
Embedding Generation: Generates embeddings for each text chunk using a Hugging Face model.
Vector Store Creation: Stores the embeddings in a FAISS vector store for efficient similarity search.
Model Initialization: Loads the Code Llama model for language understanding.
User Interaction: Provides a web interface where users can input questions.
Answer Generation: Retrieves relevant documents and generates answers using Code Llama.
Requirements
Python 3.8 or higher
Hardware:
For the 7B Code Llama model: At least 13GB of GPU VRAM or sufficient CPU RAM.
For larger models: More VRAM or RAM is required.
Libraries:
streamlit
langchain
transformers
torch
faiss-cpu or faiss-gpu
sentence-transformers
Installation
Follow these steps to set up the project:

Clone the Repository:
git clone <repository_url>
Navigate to the Project Directory:
cd <project_directory>
Create a Virtual Environment (Optional but Recommended):

# For virtualenv
python -m venv venv
source venv/bin/activate

# For conda
conda create -n knowledge_base_env python=3.9
conda activate knowledge_base_env
            
Install Required Libraries:

pip install streamlit langchain transformers torch faiss-cpu sentence-transformers
            
If you have a GPU with CUDA support, install PyTorch and FAISS with CUDA:


# Install PyTorch with CUDA support
pip install torch --extra-index-url https://download.pytorch.org/whl/cu117

# Install FAISS with GPU support
pip install faiss-gpu
            
Ensure CUDA Drivers are Installed (If Using GPU):
Make sure that CUDA drivers compatible with your GPU are installed on your system.

Download the Code Llama Model:
The model will be automatically downloaded the first time you run the application. Ensure you have sufficient disk space and a stable internet connection.

Usage
To run the application:

streamlit run main.py
Open the URL provided by Streamlit (usually http://localhost:8501Links to an external site.) in your web browser.

The application will:

Load all readable files from the current directory.
Build the knowledge base.
Initialize the Code Llama model.
Once ready, you can enter questions in the input box and receive answers based on your project's content.

Technical Details
File Loading and Preprocessing
The application uses Python's os.walk to recursively traverse directories and read files as text. Files that cannot be read as text are skipped, with warnings logged.

Document Splitting
Documents are split into chunks using LangChain's RecursiveCharacterTextSplitter with a chunk size of 1000 characters and an overlap of 100 characters.

Embedding Generation
Embeddings are generated using Hugging Face's sentence-transformers/all-MiniLM-L6-v2 model via HuggingFaceEmbeddings.

Vector Store
The embeddings are stored in a FAISS vector store, enabling efficient similarity search during retrieval.

Code Llama Integration
The application loads the Code Llama model using Hugging Face Transformers and creates a text-generation pipeline. It uses the HuggingFacePipeline from LangChain to integrate the pipeline into the retrieval QA chain.

Question Answering
When a user inputs a question, the application:

Retrieves relevant documents from the vector store using similarity search.
Uses the Code Llama model to generate an answer based on the retrieved documents.
Displays the answer and snippets from the source documents.
Notes
Model Size: The default model is Code Llama 7B. Adjust the model_name variable in the script if you wish to use a different model.
Hardware Requirements: Larger models require more VRAM or RAM. Ensure your hardware can support the model you choose.
Performance: Running on GPU significantly improves performance. CPU inference may be slow for large models.
Licensing: Ensure compliance with Code Llama's licensing terms when using the model.
License
This project is open-source and available under the MIT License. See the LICENSE file for more details.
